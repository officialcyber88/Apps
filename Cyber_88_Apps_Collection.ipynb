{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9v7Br2Rd8D0j34IkaSrB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/officialcyber88/Apps/blob/main/Cyber_88_Apps_Collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8TtMNs80KTM",
        "outputId": "adea6332-6f60-47b1-afe3-a4abb0889c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# @title Mount Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hugging Face to GitHub Transfer UI\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import gradio as gr\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def install(package):\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "install(\"gradio\")\n",
        "install(\"python-dotenv\")\n",
        "install(\"requests\")\n",
        "\n",
        "def run(cmd):\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    log = f\"$ {cmd}\\n\"\n",
        "    if result.stdout:\n",
        "        log += result.stdout\n",
        "    if result.stderr:\n",
        "        log += result.stderr\n",
        "    if result.returncode != 0:\n",
        "        raise RuntimeError(log)\n",
        "    return log\n",
        "\n",
        "def create_github_repo(token, username, repo_name, private):\n",
        "    url = \"https://api.github.com/user/repos\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Accept\": \"application/vnd.github+json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"name\": repo_name,\n",
        "        \"private\": private == \"private\",\n",
        "        \"auto_init\": False\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    if response.status_code == 201:\n",
        "        return f\"‚úÖ GitHub repository '{repo_name}' created.\\n\"\n",
        "    elif response.status_code == 422 and \"already exists\" in response.text:\n",
        "        return f\"‚ö†Ô∏è Repo '{repo_name}' already exists. Continuing...\\n\"\n",
        "    else:\n",
        "        error_msg = response.json().get(\"message\", \"Unknown error\")\n",
        "        docs_url = response.json().get(\"documentation_url\", \"\")\n",
        "        raise RuntimeError(f\"‚ùå GitHub repo creation failed: {error_msg}\\nDocumentation: {docs_url}\")\n",
        "\n",
        "def validate_github_token(token):\n",
        "    \"\"\"Validate the GitHub token with detailed error handling\"\"\"\n",
        "    url = \"https://api.github.com/user\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Accept\": \"application/vnd.github+json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise RuntimeError(f\"‚ùå Network error during token validation: {str(e)}\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Check token scopes\n",
        "        scopes = response.headers.get('X-OAuth-Scopes', '')\n",
        "        if 'repo' not in scopes:\n",
        "            return response.json()[\"login\"], f\"‚ö†Ô∏è Token missing 'repo' scope. Your scopes: {scopes}\"\n",
        "        return response.json()[\"login\"], None\n",
        "    else:\n",
        "        # Handle specific GitHub error codes\n",
        "        if response.status_code == 401:\n",
        "            error_msg = \"Token is invalid or expired\"\n",
        "        elif response.status_code == 403:\n",
        "            # Check for rate limiting\n",
        "            rate_limit = response.headers.get('X-RateLimit-Limit', '?')\n",
        "            rate_remaining = response.headers.get('X-RateLimit-Remaining', '?')\n",
        "            reset_time = response.headers.get('X-RateLimit-Reset', '?')\n",
        "            error_msg = (f\"API rate limit exceeded (Remaining: {rate_remaining}/{rate_limit}, \"\n",
        "                         f\"Resets at: {reset_time})\")\n",
        "        else:\n",
        "            try:\n",
        "                error_json = response.json()\n",
        "                error_msg = error_json.get('message', f\"HTTP error {response.status_code}\")\n",
        "                if 'documentation_url' in error_json:\n",
        "                    error_msg += f\"\\nDocumentation: {error_json['documentation_url']}\"\n",
        "            except:\n",
        "                error_msg = f\"HTTP error {response.status_code}\"\n",
        "\n",
        "        # Provide troubleshooting tips\n",
        "        tips = (\n",
        "            \"\\n\\nüîß Troubleshooting Tips:\\n\"\n",
        "            \"1. Verify token has 'repo' scope: https://github.com/settings/tokens\\n\"\n",
        "            \"2. Check token expiration: https://github.com/settings/tokens\\n\"\n",
        "            \"3. For classic tokens, ensure 'repo' scope is selected\\n\"\n",
        "            \"4. Try regenerating your token\"\n",
        "        )\n",
        "\n",
        "        raise RuntimeError(f\"‚ùå Invalid GitHub token: {error_msg}{tips}\")\n",
        "\n",
        "def transfer_repo(\n",
        "    hf_repo_url, github_username, github_repo_name, github_token,\n",
        "    git_user_name, git_user_email, branch, token_file, visibility):\n",
        "\n",
        "    try:\n",
        "        # Load from token.env if uploaded\n",
        "        if token_file:\n",
        "            env_path = Path(\"token.env\")\n",
        "            shutil.copy(token_file.name, env_path)\n",
        "            load_dotenv(dotenv_path=env_path)\n",
        "            github_token = os.getenv(\"GITHUB_TOKEN\", github_token)\n",
        "            github_username = os.getenv(\"GITHUB_USERNAME\", github_username)\n",
        "            git_user_name = os.getenv(\"GIT_USER_NAME\", git_user_name)\n",
        "            git_user_email = os.getenv(\"GIT_USER_EMAIL\", git_user_email)\n",
        "            visibility = os.getenv(\"REPO_VISIBILITY\", visibility)\n",
        "            branch = os.getenv(\"GIT_BRANCH\", branch)\n",
        "\n",
        "        # Strip any accidental whitespace from token\n",
        "        github_token = github_token.strip()\n",
        "\n",
        "        # Validate GitHub token before proceeding\n",
        "        try:\n",
        "            validated_username, scope_warning = validate_github_token(github_token)\n",
        "            if github_username and github_username != validated_username:\n",
        "                return (f\"‚ùå Token username mismatch! Token belongs to '{validated_username}' \"\n",
        "                        f\"but you entered '{github_username}'\", None)\n",
        "            github_username = validated_username\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Token validation failed: {str(e)}\", None\n",
        "\n",
        "        # Save token.env - WITHOUT COMMENTS\n",
        "        token_env_path = \"token.env\"\n",
        "        with open(token_env_path, \"w\") as f:\n",
        "            f.write(f\"GITHUB_TOKEN={github_token}\\n\")\n",
        "            f.write(f\"GITHUB_USERNAME={github_username}\\n\")\n",
        "            f.write(f\"GIT_USER_NAME={git_user_name}\\n\")\n",
        "            f.write(f\"GIT_USER_EMAIL={git_user_email}\\n\")\n",
        "            f.write(f\"REPO_VISIBILITY={visibility}\\n\")\n",
        "            f.write(f\"GIT_BRANCH={branch}\\n\")\n",
        "\n",
        "        hf_repo_url = hf_repo_url.strip().replace(\"git clone\", \"\").strip().rstrip(\"/\")\n",
        "        repo_name = hf_repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n",
        "        github_repo_name = github_repo_name.strip().lower().replace(\" \", \"-\")\n",
        "        github_repo_url = f\"https://{github_token}@github.com/{github_username}/{github_repo_name}.git\"\n",
        "\n",
        "        log = \"\"\n",
        "        log += scope_warning + \"\\n\" if scope_warning else \"\"\n",
        "        log += create_github_repo(github_token, github_username, github_repo_name, visibility)\n",
        "\n",
        "        if os.path.exists(repo_name):\n",
        "            shutil.rmtree(repo_name)\n",
        "\n",
        "        log += run(f\"git clone {hf_repo_url}\")\n",
        "        os.chdir(repo_name)\n",
        "        log += run(f'git config user.name \"{git_user_name}\"')\n",
        "        log += run(f'git config user.email \"{git_user_email}\"')\n",
        "        log += run(\"git remote rename origin hf-origin\")\n",
        "        log += run(f\"git remote add origin {github_repo_url}\")\n",
        "\n",
        "        try:\n",
        "            log += run(f\"git pull --rebase origin {branch}\")\n",
        "        except RuntimeError as e:\n",
        "            if \"couldn't find remote ref\" in str(e):\n",
        "                log += f\"‚ö†Ô∏è Remote branch '{branch}' not found. Skipping pull.\\n\"\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        log += run(f\"git push -u origin {branch}\")\n",
        "        os.chdir(\"..\")\n",
        "\n",
        "        # Success summary with minimal details\n",
        "        summary = (\n",
        "            \"‚úÖ Transfer complete!\\n\\n\"\n",
        "            f\"**Hugging Face URL:** {hf_repo_url}\\n\"\n",
        "            f\"**New GitHub Repo:** {github_repo_name}\\n\"\n",
        "            f\"**GitHub URL:** https://github.com/{github_username}/{github_repo_name}\\n\"\n",
        "            f\"**Branch:** {branch}\\n\"\n",
        "            f\"**Visibility:** {visibility}\\n\\n\"\n",
        "            \"‚¨áÔ∏è Download your token.env file below for future use.\"\n",
        "        )\n",
        "\n",
        "        return summary, token_env_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\", None\n",
        "\n",
        "def toggle_inputs(token_file):\n",
        "    if token_file:\n",
        "        return (\n",
        "            gr.update(visible=True),  # hf_repo_url\n",
        "            gr.update(visible=True),  # github_repo_name\n",
        "            gr.update(visible=False), # github_username\n",
        "            gr.update(visible=False), # github_token\n",
        "            gr.update(visible=False), # git_user_name\n",
        "            gr.update(visible=False), # git_user_email\n",
        "            gr.update(visible=True),  # branch\n",
        "            gr.update(visible=True)   # visibility\n",
        "        )\n",
        "    else:\n",
        "        return tuple([gr.update(visible=True)] * 8)\n",
        "\n",
        "def launch_app():\n",
        "    with gr.Blocks(title=\"Hugging Face to GitHub Transfer\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# Hugging Face ‚Üí GitHub Transfer Tool\")\n",
        "\n",
        "        gr.Markdown(\n",
        "            \"Transfer a Hugging Face model repo to GitHub.\\n\\n\"\n",
        "            \"**Your credentials are only stored locally in a `.env` file.**\\n\\n\"\n",
        "            \"### üîê Create a `token.env` file manually (optional)\\n\"\n",
        "            \"```ini\\n\"\n",
        "            \"GITHUB_TOKEN=your_github_token\\n\"\n",
        "            \"GITHUB_USERNAME=your_username\\n\"\n",
        "            \"GIT_USER_NAME=Your Name\\n\"\n",
        "            \"GIT_USER_EMAIL=you@example.com\\n\"\n",
        "            \"REPO_VISIBILITY=public\\n\"\n",
        "            \"GIT_BRANCH=main\\n\"\n",
        "            \"```\\n\\n\"\n",
        "            \"### üîë GitHub Token Requirements\\n\"\n",
        "            \"- Create at: https://github.com/settings/tokens\\n\"\n",
        "            \"- Must be a **classic token** with **repo scope**\\n\"\n",
        "            \"- Token must start with `ghp_` (40 characters)\\n\"\n",
        "            \"- Required permissions: `repo` (full control)\\n\"\n",
        "            \"- Never share your token with anyone!\\n\"\n",
        "            \"- Token expires after 30 days by default (set expiration to 'No expiration')\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            token_file = gr.File(label=\"Upload token.env (optional)\", file_types=[\".env\"])\n",
        "\n",
        "        with gr.Column():\n",
        "            hf_repo_url = gr.Textbox(label=\"Hugging Face Repo URL (.git)\",\n",
        "                                     placeholder=\"https://huggingface.co/username/repo_name\")\n",
        "            github_repo_name = gr.Textbox(label=\"New GitHub Repo Name\",\n",
        "                                         placeholder=\"my-new-repo\")\n",
        "\n",
        "        with gr.Row():\n",
        "            github_username = gr.Textbox(label=\"GitHub Username\",\n",
        "                                        placeholder=\"your-github-username\")\n",
        "            github_token = gr.Textbox(label=\"GitHub Token\", type=\"password\",\n",
        "                                     placeholder=\"ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
        "\n",
        "        with gr.Row():\n",
        "            git_user_name = gr.Textbox(label=\"Git Commit Author Name\", value=\"Your Name\")\n",
        "            git_user_email = gr.Textbox(label=\"Git Commit Author Email\", value=\"you@example.com\")\n",
        "\n",
        "        with gr.Row():\n",
        "            branch = gr.Dropdown([\"main\", \"dev\", \"test\", \"custom\"], label=\"GitHub Branch\", value=\"main\")\n",
        "            visibility = gr.Dropdown([\"public\", \"private\"], label=\"Visibility\", value=\"public\")\n",
        "\n",
        "        run_btn = gr.Button(\"Start Transfer\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output = gr.Textbox(label=\"Transfer Log\", lines=10, interactive=False, show_copy_button=True)\n",
        "            token_download = gr.File(label=\"Download token.env\", visible=False, interactive=True)\n",
        "\n",
        "        token_file.change(\n",
        "            fn=toggle_inputs,\n",
        "            inputs=[token_file],\n",
        "            outputs=[\n",
        "                hf_repo_url, github_repo_name,\n",
        "                github_username, github_token,\n",
        "                git_user_name, git_user_email,\n",
        "                branch, visibility\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        run_btn.click(\n",
        "            fn=transfer_repo,\n",
        "            inputs=[\n",
        "                hf_repo_url, github_username, github_repo_name,\n",
        "                github_token, git_user_name, git_user_email,\n",
        "                branch, token_file, visibility\n",
        "            ],\n",
        "            outputs=[output, token_download]\n",
        "        )\n",
        "\n",
        "        # Display the token.env download link immediately after transfer\n",
        "        token_download.change(\n",
        "            lambda x: gr.update(visible=x is not None),\n",
        "            inputs=[token_download],\n",
        "            outputs=[token_download]\n",
        "        )\n",
        "\n",
        "        demo.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    launch_app()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UTdgyE91jXuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title File and Directory inspector UI\n",
        "\n",
        "import sys\n",
        "import tempfile\n",
        "import json\n",
        "import csv\n",
        "import traceback\n",
        "import subprocess\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import textwrap\n",
        "\n",
        "# Auto-install PyYAML if needed\n",
        "try:\n",
        "    import yaml\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyyaml\"])\n",
        "    import yaml\n",
        "\n",
        "# Ensure Gradio is present\n",
        "try:\n",
        "    import gradio as gr\n",
        "except ImportError:\n",
        "    print(\"Please install gradio: pip install gradio\", file=sys.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "# No exclusions‚Äîaccept all files\n",
        "_EXCLUDE_EXTS = set()\n",
        "\n",
        "# Syntax highlighting mapping for Markdown\n",
        "_SYNTAX_MAP = {\n",
        "    '.py': 'python', '.js': 'javascript', '.mjs': 'javascript',\n",
        "    '.html': 'html', '.css': 'css', '.json': 'json',\n",
        "    '.yml': 'yaml', '.yaml': 'yaml', '.md': 'markdown',\n",
        "    '.sh': 'bash', '.java': 'java', '.c': 'c', '.cpp': 'cpp',\n",
        "    '.h': 'cpp', '.cs': 'csharp', '.php': 'php', '.rb': 'ruby',\n",
        "    '.go': 'go', '.rs': 'rust', '.swift': 'swift', '.kt': 'kotlin',\n",
        "    '.ts': 'typescript', '.sql': 'sql', '.xml': 'xml',\n",
        "    '.svg': 'xml', '.ini': 'ini', '.cfg': 'ini',\n",
        "    '.toml': 'toml', '.lock': 'json',\n",
        "}\n",
        "\n",
        "def human_readable_size(num, suffix='B'):\n",
        "    for unit in ['','K','M','G','T','P','E','Z']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return f\"{num:3.1f}{unit}{suffix}\"\n",
        "        num /= 1024.0\n",
        "    return f\"{num:.1f}Y{suffix}\"\n",
        "\n",
        "def is_text_file(path: Path, blocksize: int = 1024) -> bool:\n",
        "    try:\n",
        "        with path.open('rb') as f:\n",
        "            if b'\\0' in f.read(blocksize):\n",
        "                return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def build_tree(root: Path):\n",
        "    tree = []\n",
        "    try:\n",
        "        root_name = root.name or \"root_directory\"\n",
        "        tree.append({\n",
        "            \"path\": \".\",\n",
        "            \"is_dir\": True,\n",
        "            \"name\": root_name,\n",
        "            \"size\": 0,\n",
        "            \"hr_size\": human_readable_size(0),\n",
        "            \"modified\": root.stat().st_mtime\n",
        "        })\n",
        "        for p in root.rglob('*'):\n",
        "            if p == root:\n",
        "                continue\n",
        "            try:\n",
        "                rel = p.relative_to(root)\n",
        "                stat = p.stat()\n",
        "                tree.append({\n",
        "                    \"path\": str(rel),\n",
        "                    \"is_dir\": p.is_dir(),\n",
        "                    \"name\": p.name,\n",
        "                    \"size\": stat.st_size,\n",
        "                    \"hr_size\": human_readable_size(stat.st_size),\n",
        "                    \"modified\": stat.st_mtime\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {p}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error building tree: {e}\")\n",
        "        traceback.print_exc()\n",
        "    return tree\n",
        "\n",
        "def build_files(root: Path, max_workers: int = None):\n",
        "    max_workers = max_workers or (os.cpu_count() or 4)\n",
        "    all_files = [p for p in root.rglob('*') if p.is_file()]\n",
        "\n",
        "    def read_and_clean(p):\n",
        "        if not is_text_file(p):\n",
        "            return None\n",
        "        try:\n",
        "            text = p.read_text(encoding='utf-8', errors='replace')\n",
        "            lines = []\n",
        "            for line in text.splitlines():\n",
        "                if len(line) > 120:\n",
        "                    lines.extend(textwrap.wrap(line, width=120, break_long_words=False))\n",
        "                else:\n",
        "                    lines.append(line)\n",
        "            return \"\\n\".join(lines)\n",
        "        except Exception as e:\n",
        "            return f\"Error reading {p}: {e}\"\n",
        "\n",
        "    files = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as exe:\n",
        "        futures = {exe.submit(read_and_clean, p): p for p in all_files}\n",
        "        for fut in as_completed(futures):\n",
        "            content = fut.result()\n",
        "            if content is not None:\n",
        "                p = futures[fut]\n",
        "                files.append({\n",
        "                    \"path\": str(p.relative_to(root)),\n",
        "                    \"content\": content,\n",
        "                    \"syntax\": _SYNTAX_MAP.get(p.suffix.lower(), '')\n",
        "                })\n",
        "    return files\n",
        "\n",
        "def export_txt(tree, files):\n",
        "    parts = []\n",
        "    if tree:\n",
        "        parts.append(\"Directory Structure:\")\n",
        "        for e in tree:\n",
        "            indent = '    ' * e['path'].count(os.sep)\n",
        "            parts.append(f\"{indent}{e['name']} ({e['hr_size']}): {e['path']}\")\n",
        "    if files:\n",
        "        parts.append(\"\\nFile Contents:\")\n",
        "        for f in files:\n",
        "            parts.append(f\"\\nFile: {f['path']}\\n{f['content']}\")\n",
        "    return \"\\n\".join(parts) or \"No content found\"\n",
        "\n",
        "def export_json(tree, files):\n",
        "    return json.dumps({\n",
        "        \"metadata\": {\n",
        "            \"directory\": {\n",
        "                \"name\": tree[0]['name'] if tree else \"\",\n",
        "                \"file_count\": len(files),\n",
        "                \"directory_count\": sum(1 for t in tree if t['is_dir'])\n",
        "            }\n",
        "        },\n",
        "        \"structure\": tree,\n",
        "        \"files\": files\n",
        "    }, indent=2)\n",
        "\n",
        "def export_jsonl(tree, files):\n",
        "    lines = []\n",
        "    if tree:\n",
        "        for entry in tree:\n",
        "            lines.append(json.dumps({\n",
        "                \"type\": \"structure\",\n",
        "                \"data\": entry\n",
        "            }))\n",
        "    if files:\n",
        "        for file_entry in files:\n",
        "            lines.append(json.dumps({\n",
        "                \"type\": \"content\",\n",
        "                \"data\": file_entry\n",
        "            }))\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def export_yaml(tree, files):\n",
        "    return yaml.dump({\n",
        "        \"metadata\": {\n",
        "            \"directory\": {\n",
        "                \"name\": tree[0]['name'] if tree else \"\",\n",
        "                \"file_count\": len(files),\n",
        "                \"directory_count\": sum(1 for t in tree if t['is_dir'])\n",
        "            }\n",
        "        },\n",
        "        \"structure\": tree,\n",
        "        \"files\": files\n",
        "    }, sort_keys=False, allow_unicode=True)\n",
        "\n",
        "def export_markdown(tree, files):\n",
        "    md = []\n",
        "    if tree:\n",
        "        md.append(\"# Directory Structure\")\n",
        "        for e in tree:\n",
        "            indent = '  ' * e['path'].count(os.sep)\n",
        "            md.append(f\"{indent}- **{e['name']}** ({e['hr_size']}): `{e['path']}`\")\n",
        "    if files:\n",
        "        md.append(\"\\n# File Contents\")\n",
        "        for f in files:\n",
        "            lang = f['syntax'] or 'text'\n",
        "            md.append(f\"\\n## {f['path']}\\n```{lang}\\n{f['content']}\\n```\")\n",
        "    return \"\\n\".join(md) or \"No content found\"\n",
        "\n",
        "def export_csv(tree, files):\n",
        "    from io import StringIO\n",
        "    out = StringIO()\n",
        "    w = csv.writer(out)\n",
        "    w.writerow([\"Type\",\"Path\",\"Size\",\"Modified\"])\n",
        "    for e in tree:\n",
        "        w.writerow([\n",
        "            \"DIR\" if e['is_dir'] else \"FILE\",\n",
        "            e['path'], e['size'], e['modified']\n",
        "        ])\n",
        "    if files:\n",
        "        w.writerow([])\n",
        "        w.writerow([\"File Path\",\"Content Excerpt\"])\n",
        "        for f in files:\n",
        "            excerpt = (f['content'][:200] + '...') if len(f['content'])>200 else f['content']\n",
        "            w.writerow([f['path'], excerpt])\n",
        "    return out.getvalue()\n",
        "\n",
        "def export_tsv(tree, files):\n",
        "    from io import StringIO\n",
        "    out = StringIO()\n",
        "    w = csv.writer(out, delimiter='\\t')\n",
        "    w.writerow([\"Type\",\"Path\",\"Size\",\"Modified\"])\n",
        "    for e in tree:\n",
        "        w.writerow([\n",
        "            \"DIR\" if e['is_dir'] else \"FILE\",\n",
        "            e['path'], e['size'], e['modified']\n",
        "        ])\n",
        "    if files:\n",
        "        w.writerow([])\n",
        "        w.writerow([\"File Path\",\"Content Excerpt\"])\n",
        "        for f in files:\n",
        "            excerpt = (f['content'][:200] + '...') if len(f['content'])>200 else f['content']\n",
        "            w.writerow([f['path'], excerpt])\n",
        "    return out.getvalue()\n",
        "\n",
        "def export_html(tree, files):\n",
        "    lines = ['<html><head><meta charset=\"utf-8\"><title>File Inspector Report</title></head><body>']\n",
        "    lines.append(f\"<h1>Directory: {tree[0]['name'] if tree else ''}</h1>\")\n",
        "    if tree:\n",
        "        lines.append(\"<details open><summary>Structure</summary><pre>\")\n",
        "        for e in tree:\n",
        "            lines.append(f\"{e['path']} ({e['hr_size']})\")\n",
        "        lines.append(\"</pre></details>\")\n",
        "    if files:\n",
        "        lines.append(\"<details><summary>File Contents</summary>\")\n",
        "        for f in files:\n",
        "            lines.append(f\"<h2>{f['path']}</h2><pre>{f['content']}</pre>\")\n",
        "        lines.append(\"</details>\")\n",
        "    lines.append(\"</body></html>\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "_FORMATS = {\n",
        "    \"TXT\":      {\"ext\":\"txt\",  \"exporter\":export_txt,      \"desc\":\"Plain text. Quick & simple.\"},\n",
        "    \"JSON\":     {\"ext\":\"json\", \"exporter\":export_json,     \"desc\":\"Structured JSON for APIs.\"},\n",
        "    \"JSONL\":    {\"ext\":\"jsonl\",\"exporter\":export_jsonl,    \"desc\":\"JSON Lines for streaming/NDJSON.\"},\n",
        "    \"YAML\":     {\"ext\":\"yaml\", \"exporter\":export_yaml,     \"desc\":\"Human-friendly YAML.\"},\n",
        "    \"Markdown\": {\"ext\":\"md\",   \"exporter\":export_markdown, \"desc\":\"Markdown with syntax coloring.\"},\n",
        "    \"CSV\":      {\"ext\":\"csv\",  \"exporter\":export_csv,      \"desc\":\"CSV for spreadsheets.\"},\n",
        "    \"TSV\":      {\"ext\":\"tsv\",  \"exporter\":export_tsv,      \"desc\":\"TSV for tab-delimited data.\"},\n",
        "    \"HTML\":     {\"ext\":\"html\",\"exporter\":export_html,      \"desc\":\"Interactive HTML report.\"}\n",
        "}\n",
        "\n",
        "def process_and_save(source: str, local_path: str, zip_file, action: str, fmt: str):\n",
        "    try:\n",
        "        if source == \"Upload ZIP\":\n",
        "            if zip_file is None:\n",
        "                return None, \"Error: no ZIP uploaded\"\n",
        "            tmpdir = Path(tempfile.mkdtemp())\n",
        "            with zipfile.ZipFile(zip_file.name, 'r') as z:\n",
        "                z.extractall(tmpdir)\n",
        "            root = tmpdir\n",
        "        else:\n",
        "            root = Path(local_path).expanduser().resolve()\n",
        "\n",
        "        if not root.is_dir():\n",
        "            return None, \"Error: Invalid directory\"\n",
        "\n",
        "        tree  = build_tree(root)  if \"Tree\" in action else []\n",
        "        files = build_files(root) if \"Extract Code\" in action else []\n",
        "\n",
        "        if fmt not in _FORMATS:\n",
        "            return None, f\"Error: Unknown format {fmt}\"\n",
        "        exporter = _FORMATS[fmt][\"exporter\"]\n",
        "        content  = exporter(tree, files)\n",
        "\n",
        "        out_name = f\"{root.name or 'output'}.{_FORMATS[fmt]['ext']}\"\n",
        "        out_path = Path(tempfile.gettempdir()) / out_name\n",
        "        out_path.write_text(content, encoding='utf-8')\n",
        "        return str(out_path), content\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Processing error: {e}\\n{traceback.format_exc()}\"\n",
        "\n",
        "def explain_format(fmt: str):\n",
        "    info = _FORMATS.get(fmt)\n",
        "    return f\"**{fmt} Format**\\n{info['desc']}\" if info else \"Select a valid format\"\n",
        "\n",
        "def launch_app():\n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"File Inspector\") as demo:\n",
        "        gr.Markdown(\"# File Inspector Tool\")\n",
        "        gr.Markdown(\"Choose a local folder or upload a ZIP archive, then view/export its contents.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                source_input = gr.Dropdown(\n",
        "                    label=\"Input Source\",\n",
        "                    choices=[\"Local Path\", \"Upload ZIP\"],\n",
        "                    value=\"Local Path\"\n",
        "                )\n",
        "                dir_input = gr.Textbox(\n",
        "                    label=\"Directory Path\",\n",
        "                    placeholder=\"/path/to/project\",\n",
        "                    value=str(Path.home())\n",
        "                )\n",
        "                zip_input = gr.File(\n",
        "                    label=\"Upload ZIP Archive\",\n",
        "                    file_types=[\".zip\"],\n",
        "                    visible=False\n",
        "                )\n",
        "                action_input = gr.Dropdown(\n",
        "                    label=\"Action\",\n",
        "                    choices=[\"Display Tree\", \"Extract Code\", \"Display Tree + Extract Code\"],\n",
        "                    value=\"Display Tree + Extract Code\"\n",
        "                )\n",
        "                format_input = gr.Dropdown(\n",
        "                    label=\"Export Format\",\n",
        "                    choices=list(_FORMATS.keys()),\n",
        "                    value=\"TXT\"\n",
        "                )\n",
        "                fmt_explain = gr.Markdown(explain_format(\"TXT\"))\n",
        "                run_btn = gr.Button(\"Run\", variant=\"primary\")\n",
        "                download = gr.File(label=\"Download Output\", interactive=False)\n",
        "\n",
        "            with gr.Column(scale=3):\n",
        "                content_box = gr.TextArea(\n",
        "                    label=\"Output\",\n",
        "                    lines=20,\n",
        "                    interactive=False,\n",
        "                    elem_id=\"output_txt\",\n",
        "                    elem_classes=[\"output-box\"],\n",
        "                    show_copy_button=True  # <-- use built-in copy\n",
        "                )\n",
        "\n",
        "        source_input.change(\n",
        "            fn=lambda src: (\n",
        "                gr.update(visible=(src==\"Local Path\")),\n",
        "                gr.update(visible=(src==\"Upload ZIP\"))\n",
        "            ),\n",
        "            inputs=source_input,\n",
        "            outputs=[dir_input, zip_input]\n",
        "        )\n",
        "\n",
        "        format_input.change(\n",
        "            fn=explain_format,\n",
        "            inputs=format_input,\n",
        "            outputs=fmt_explain\n",
        "        )\n",
        "\n",
        "        run_btn.click(\n",
        "            fn=process_and_save,\n",
        "            inputs=[source_input, dir_input, zip_input, action_input, format_input],\n",
        "            outputs=[download, content_box]\n",
        "        )\n",
        "\n",
        "        demo.css = \"\"\"\n",
        "        .output-box textarea {\n",
        "            font-family: 'Courier New', monospace !important;\n",
        "            white-space: pre !important;\n",
        "            user-select: text !important;\n",
        "            -webkit-user-select: text !important;\n",
        "        }\n",
        "        \"\"\"\n",
        "        demo.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    launch_app()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Wh0fMjy6s2Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Audio Preprocessor UI\n",
        "\n",
        "import sys\n",
        "import shutil\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import tempfile\n",
        "import warnings\n",
        "import pyloudnorm as pyln\n",
        "import subprocess\n",
        "from dataclasses import dataclass\n",
        "import gradio as gr\n",
        "import traceback\n",
        "from uuid import uuid4\n",
        "import zipfile\n",
        "import concurrent.futures\n",
        "\n",
        "# === Pre-flight Check: Install missing dependencies if needed ===\n",
        "try:\n",
        "    import resampy\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"resampy\"])\n",
        "    import resampy\n",
        "\n",
        "try:\n",
        "    import gdown\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gdown\"])\n",
        "    import gdown\n",
        "\n",
        "# === Dependency Check ===\n",
        "if shutil.which(\"ffmpeg\") is None:\n",
        "    sys.stderr.write(\"Missing dependency: ffmpeg\\n\")\n",
        "    sys.exit(1)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# === Global Setup ===\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "OUTPUT_DIR = tempfile.mkdtemp(prefix=\"audio_preprocessor_\")\n",
        "print(f\"Audio output will be saved in: {OUTPUT_DIR}\")\n",
        "\n",
        "# === Constants ===\n",
        "DB_THRESH = -45\n",
        "EDGE_SILENCE_THRESHOLD = 3e-3\n",
        "VALID_FORMATS = ('.wav', '.mp3', '.flac', '.aiff', '.ogg', '.m4a')\n",
        "MIN_FRAMES_FOR_RMS = 50\n",
        "DEFAULT_HOP_LENGTH = 512\n",
        "\n",
        "# === Config Dataclass ===\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    sample_rate: int\n",
        "    bit_depth: str\n",
        "    channels: str\n",
        "    target_lufs: float\n",
        "    target_peak: float\n",
        "    use_cuda: bool\n",
        "    device: torch.device\n",
        "    visualize: bool\n",
        "    segmentation: bool\n",
        "    duration: float\n",
        "    panning: bool\n",
        "    mp3_bitrate: str\n",
        "    silence_trimming: bool  # Added silence trimming flag\n",
        "\n",
        "# === Helpers ===\n",
        "\n",
        "def pan_percent(l, r):\n",
        "    pl, pr = np.sum(l**2), np.sum(r**2)\n",
        "    t = pl + pr\n",
        "    if t < 1e-10:\n",
        "        return 50.0, 50.0, 0.0\n",
        "    return pl/t*100, pr/t*100, abs(pl/t*100 - 50)\n",
        "\n",
        "def calculate_adaptive_hop_length(L):\n",
        "    return min(DEFAULT_HOP_LENGTH, max(32, L // MIN_FRAMES_FOR_RMS))\n",
        "\n",
        "def measure_loudness(y, sr):\n",
        "    BLOCK = 0.4\n",
        "    if y.size == 0:\n",
        "        return {'lufs': None, 'peak': -np.inf}\n",
        "    y_m = np.mean(y, axis=0) if y.ndim > 1 else y\n",
        "    min_len = int(BLOCK * sr)\n",
        "    if len(y_m) < min_len:\n",
        "        y_m = np.pad(y_m, (0, min_len - len(y_m)))\n",
        "    meter = pyln.Meter(sr, block_size=BLOCK)\n",
        "    try:\n",
        "        lufs = meter.integrated_loudness(y_m)\n",
        "    except:\n",
        "        lufs = None\n",
        "    pk = np.max(np.abs(y))\n",
        "    pdb = 20 * np.log10(pk) if pk > 0 else -np.inf\n",
        "    return {'lufs': lufs, 'peak': pdb}\n",
        "\n",
        "def normalize_loudness(y, sr, log, tgt_lufs, tgt_peak):\n",
        "    if y.size == 0:\n",
        "        return y, {'method': 'empty'}\n",
        "    orig_pk = np.max(np.abs(y))\n",
        "    orig_db = 20 * np.log10(orig_pk) if orig_pk > 0 else -np.inf\n",
        "    y_m = np.mean(y, axis=0) if y.ndim > 1 else y\n",
        "    BLOCK = 0.4\n",
        "    min_len = max(int(BLOCK * sr), int(0.001 * sr))\n",
        "    if len(y_m) < min_len:\n",
        "        log(f\"[Normalize] Padding {min_len - len(y_m)} samples\")\n",
        "        y_m = np.pad(y_m, (0, min_len - len(y_m)))\n",
        "    meter = pyln.Meter(sr)\n",
        "    try:\n",
        "        orig_lufs = meter.integrated_loudness(y_m)\n",
        "        log(f\"[Normalize] Orig LUFS {orig_lufs:.2f}, Peak {orig_db:.2f}\")\n",
        "    except Exception as e:\n",
        "        log(f\"[Normalize] LUFS failed ({e}), peak-only\")\n",
        "        scale = (10 ** (tgt_peak / 20)) / orig_pk if orig_pk > 0 else 1\n",
        "        y_n = y * scale\n",
        "        fp = 20 * np.log10(np.max(np.abs(y_n))) if np.max(np.abs(y_n)) > 0 else -np.inf\n",
        "        return y_n, {\n",
        "            'original_lufs': None,\n",
        "            'original_peak': orig_db,\n",
        "            'normalized_lufs': None,\n",
        "            'normalized_peak': fp,\n",
        "            'method': 'peak_only'\n",
        "        }\n",
        "    gain = 10 ** ((tgt_lufs - orig_lufs) / 20)\n",
        "    y_l = y * gain\n",
        "    pk_after = np.max(np.abs(y_l))\n",
        "    if pk_after > 10 ** (tgt_peak / 20):\n",
        "        log(\"[Normalize] Limiting peak\")\n",
        "        y_n = y_l * (10 ** (tgt_peak / 20) / pk_after)\n",
        "    else:\n",
        "        y_n = y_l\n",
        "    fl = measure_loudness(y_n, sr)['lufs']\n",
        "    fp = 20 * np.log10(np.max(np.abs(y_n))) if np.max(np.abs(y_n)) > 0 else -np.inf\n",
        "    log(f\"[Normalize] Final LUFS {fl:.2f}, Peak {fp:.2f}\")\n",
        "    return y_n, {\n",
        "        'original_lufs': orig_lufs,\n",
        "        'original_peak': orig_db,\n",
        "        'normalized_lufs': fl,\n",
        "        'normalized_peak': fp,\n",
        "        'method': 'true_lufs'\n",
        "    }\n",
        "\n",
        "def normalize_panning(a, log):\n",
        "    if a.ndim != 2 or a.shape[0] != 2:\n",
        "        return a\n",
        "    lp, rp, _ = pan_percent(a[0], a[1])\n",
        "    log(f\"[Panning] Orig L{lp:.1f}% R{rp:.1f}%\")\n",
        "    r1, r2 = np.sqrt(np.mean(a[0]**2)), np.sqrt(np.mean(a[1]**2))\n",
        "    if r1 < 1e-7 or r2 < 1e-7:\n",
        "        return a\n",
        "    corr = np.vstack((a[0], a[1] * (r1 / r2)))\n",
        "    lp2, rp2, _ = pan_percent(corr[0], corr[1])\n",
        "    log(f\"[Panning] Corr L{lp2:.1f}% R{rp2:.1f}%\")\n",
        "    return corr\n",
        "\n",
        "def detect_clipping(y):\n",
        "    if y.size == 0:\n",
        "        return False, 0.0\n",
        "    c = np.sum(np.abs(y) >= 0.999) / y.size\n",
        "    return c > 0.001, c\n",
        "\n",
        "def attenuate_clipped_audio(y, log):\n",
        "    clip, ratio = detect_clipping(y)\n",
        "    if clip:\n",
        "        log(f\"[Clipping] {ratio:.1%} clipped; attenuating\")\n",
        "        pk = np.max(np.abs(y))\n",
        "        tgt = 10 ** (-1 / 20)\n",
        "        if pk > 0:\n",
        "            return y * (tgt / pk)\n",
        "    return y\n",
        "\n",
        "def auto_slice_audio(y, sr):\n",
        "    if y.size == 0:\n",
        "        return 0, 0\n",
        "    L = y.shape[-1]\n",
        "    if L < 128:\n",
        "        return 0, L\n",
        "    hop = calculate_adaptive_hop_length(L)\n",
        "    frame = min(2048, L)\n",
        "    rms = librosa.feature.rms(y=y, frame_length=frame, hop_length=hop)[0]\n",
        "    db = librosa.amplitude_to_db(rms, ref=np.max)\n",
        "    idx = np.where(db > DB_THRESH)[0]\n",
        "    if idx.size == 0:\n",
        "        return 0, L\n",
        "    return idx[0] * hop, min(L, (idx[-1] + 1) * hop)\n",
        "\n",
        "def process_silence(y, sr, log):\n",
        "    if y.size == 0:\n",
        "        return y, (0,0,0), (0,0)\n",
        "    L = y.shape[-1]\n",
        "    y_m = np.mean(y,axis=0) if y.ndim>1 else y\n",
        "    s0,e0 = auto_slice_audio(y_m, sr)\n",
        "    if e0<=s0:\n",
        "        log(\"[Silence] All silent\")\n",
        "        return np.array([]),(L/sr,0,L/sr),(0,0)\n",
        "    t = y[...,s0:e0]\n",
        "    tm = np.mean(t,axis=0) if t.ndim>1 else t\n",
        "    nz = np.where(np.abs(tm)>EDGE_SILENCE_THRESHOLD)[0]\n",
        "    if nz.size==0:\n",
        "        log(\"[Silence] All trimmed\")\n",
        "        return np.array([]),(L/sr,0,L/sr),(0,0)\n",
        "    fs,fe = nz[0],nz[-1]+1\n",
        "    final = t[...,fs:fe]\n",
        "    rem = L-final.shape[-1]\n",
        "    return final,((s0+fs)/sr,(L-(s0+fe))/sr,rem/sr),(s0+fs,s0+fe)\n",
        "\n",
        "def format_duration(s): return f\"{s:.3f}s\"\n",
        "\n",
        "def plot_zoomed_silence(y, sr, s0, e0, zoom=0.05):\n",
        "    zs = int(sr*zoom)\n",
        "    fig, axs = plt.subplots(2,1,figsize=(6,4))\n",
        "    pre = y[...,max(0,s0-zs):s0]\n",
        "    t0 = np.linspace(-zoom, 0, pre.shape[-1])\n",
        "    if pre.size>0: axs[0].plot(t0, pre.T if pre.ndim>1 else pre)\n",
        "    else: axs[0].text(0.5,0.5,\"No pre-silence\",ha='center')\n",
        "    axs[0].set_xlim(t0[0] if pre.size>0 else -zoom, 0)\n",
        "    axs[0].set_title(\"Zoomed Silence Pre-trim\")\n",
        "    post = y[...,e0:e0+zs]\n",
        "    t1 = np.linspace(0, zoom, post.shape[-1])\n",
        "    if post.size>0: axs[1].plot(t1, post.T if post.ndim>1 else post)\n",
        "    else: axs[1].text(0.5,0.5,\"No post-silence\",ha='center')\n",
        "    axs[1].set_xlim(0, t1[-1] if post.size>0 else zoom)\n",
        "    axs[1].set_title(\"Zoomed Silence Post-trim\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_waveform(y, sr, title, time_unit_str=\"s\", s0=None, e0=None, segments=None):\n",
        "    fig, ax = plt.subplots(figsize=(6,2.5))\n",
        "    t = np.arange(y.shape[-1])/sr\n",
        "    if y.ndim>1:\n",
        "        for c in range(y.shape[0]):\n",
        "            ax.plot(t, y[c], alpha=0.7, label=f'Ch{c+1}')\n",
        "        ax.legend(fontsize=\"small\")\n",
        "    else:\n",
        "        ax.plot(t, y)\n",
        "    if s0 is not None and e0 is not None:\n",
        "        ax.axvline(s0/sr, linestyle='--')\n",
        "        ax.axvline(e0/sr, linestyle='--')\n",
        "    if segments:\n",
        "        for st,en in segments:\n",
        "            ax.axvline(st/sr, linestyle='-', alpha=0.6)\n",
        "            ax.axvline(en/sr, linestyle='-', alpha=0.6)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(f\"Time ({time_unit_str})\")\n",
        "    ax.set_ylabel(\"Amplitude\")\n",
        "    ax.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def get_all_audio_files(path):\n",
        "    files = []\n",
        "    if os.path.isfile(path) and path.lower().endswith(VALID_FORMATS):\n",
        "        files.append(path)\n",
        "    elif os.path.isdir(path):\n",
        "        for root, _, fnames in os.walk(path):\n",
        "            for f in fnames:\n",
        "                if f.lower().endswith(VALID_FORMATS):\n",
        "                    files.append(os.path.join(root, f))\n",
        "    elif os.path.isfile(path) and path.lower().endswith('.zip'):\n",
        "        tmp = tempfile.mkdtemp(prefix=\"zip_extract_\")\n",
        "        with zipfile.ZipFile(path, 'r') as zf:\n",
        "            zf.extractall(tmp)\n",
        "        for root, _, fnames in os.walk(tmp):\n",
        "            for f in fnames:\n",
        "                if f.lower().endswith(VALID_FORMATS):\n",
        "                    files.append(os.path.join(root, f))\n",
        "    return files\n",
        "\n",
        "def download_from_gdrive_folder(url, log):\n",
        "    m = re.search(r'/folders/([^/?]+)', url)\n",
        "    if not m:\n",
        "        log(\"‚ùå URL must be a shared FOLDER link\")\n",
        "        return None, None\n",
        "    fid = m.group(1)\n",
        "    parent = tempfile.mkdtemp(prefix=\"gdrive_dl_\")\n",
        "    outdir = os.path.join(parent, fid)\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    log(f\"[GDrive] Downloading folder ID {fid} to {outdir}\")\n",
        "    gdown.download_folder(url=url, output=outdir, quiet=True, use_cookies=False)\n",
        "    subs = [d for d in os.listdir(outdir) if os.path.isdir(os.path.join(outdir, d))]\n",
        "    if len(subs)==1:\n",
        "        det = subs[0]\n",
        "        log(f\"[GDrive] Detected folder name: {det}\")\n",
        "        return outdir, det\n",
        "    return outdir, fid\n",
        "\n",
        "def export_audio(y, sr, orig, fmt, cfg, exp_dir, idx=None):\n",
        "    base = os.path.splitext(os.path.basename(orig))[0]\n",
        "    name = f\"{base}_segment_{idx+1}.{fmt}\" if idx is not None else f\"{base}.{fmt}\"\n",
        "    out = os.path.join(exp_dir, name)\n",
        "    if y.size==0:\n",
        "        ch = 2 if cfg.channels=='stereo' else 1\n",
        "        y = np.zeros((ch,1)) if ch>1 else np.zeros(1)\n",
        "    dat = y.T if y.ndim>1 else y\n",
        "    subtype = f\"PCM_{cfg.bit_depth}\"\n",
        "    if fmt==\"mp3\":\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as tmp:\n",
        "            sf.write(tmp.name, dat, sr, subtype=\"FLOAT\")\n",
        "            cmd = [\"ffmpeg\",\"-y\",\"-i\", tmp.name, \"-c:a\",\"libmp3lame\",\"-b:a\",cfg.mp3_bitrate,\"-ar\",str(sr), out]\n",
        "            subprocess.run(cmd, check=True, capture_output=True)\n",
        "    else:\n",
        "        if fmt==\"flac\" and cfg.bit_depth==\"32\":\n",
        "            subtype = \"PCM_24\"\n",
        "        sf.write(out, dat, sr, subtype=subtype)\n",
        "    return out\n",
        "\n",
        "def process_file(fp, cfg, params):\n",
        "    res = {'messages':[], 'export_paths':[], 'plot_data':None}\n",
        "    def log(m): res['messages'].append(m)\n",
        "    try:\n",
        "        log(f\"--- Processing {os.path.basename(fp)} ---\")\n",
        "        log(f\"Device: {'GPU' if cfg.use_cuda else 'CPU'}\")\n",
        "        y, orig_sr = librosa.load(fp, sr=None, mono=False)\n",
        "        if y.ndim==1: y = y[np.newaxis,:]\n",
        "        info = sf.info(fp)\n",
        "        log(f\"Input: {info.samplerate}Hz, {info.channels}ch, {format_duration(info.duration)}\")\n",
        "        if cfg.panning and y.shape[0]==2:\n",
        "            y = normalize_panning(y, log)\n",
        "        if orig_sr!=cfg.sample_rate:\n",
        "            log(f\"Resampling {orig_sr}‚Üí{cfg.sample_rate}\")\n",
        "            y = resampy.resample(y, orig_sr, cfg.sample_rate)\n",
        "        if y.ndim==1: y = y[np.newaxis,:]\n",
        "        if cfg.channels==\"mono\" and y.shape[0]>1:\n",
        "            log(\"Converting to mono\")\n",
        "            y = np.mean(y, axis=0, keepdims=True)\n",
        "        elif cfg.channels==\"stereo\" and y.shape[0]==1:\n",
        "            y = np.vstack([y,y])\n",
        "        y = attenuate_clipped_audio(y, log)\n",
        "        y_pre = y.copy()\n",
        "\n",
        "        # Apply silence trimming if enabled\n",
        "        if cfg.silence_trimming:  # New conditional check\n",
        "            y_proc, (_pre,_post,total), (s0,e0) = process_silence(y, cfg.sample_rate, log)\n",
        "            log(f\"Silence removed {format_duration(total)} (start {format_duration(_pre)}, end {format_duration(_post)})\")\n",
        "        else:\n",
        "            y_proc = y\n",
        "            s0, e0 = 0, y.shape[-1]\n",
        "            log(\"Silence trimming skipped\")\n",
        "\n",
        "        if y_proc.size==0:\n",
        "            log(\"Empty after silence; skipping\")\n",
        "            return res\n",
        "\n",
        "        segments = [(0, y_proc.shape[-1])]\n",
        "        if cfg.segmentation:\n",
        "            dur_sec = y_proc.shape[-1]/cfg.sample_rate\n",
        "            if dur_sec < cfg.duration:\n",
        "                log(\"‚ö† shorter than segment duration; skipping export\")\n",
        "                return res\n",
        "            ss = int(cfg.sample_rate * cfg.duration)\n",
        "            nseg = int(np.ceil(dur_sec / cfg.duration))\n",
        "            segments = [(i*ss, min((i+1)*ss, y_proc.shape[-1])) for i in range(nseg)]\n",
        "            log(f\"Segmenting into {format_duration(cfg.duration)}, created {len(segments)} segments\")\n",
        "\n",
        "        res['plot_data'] = {'y_pre':y_pre, 'y_proc':y_proc, 'sr':cfg.sample_rate, 's0':s0, 'e0':e0, 'segments':segments}\n",
        "\n",
        "        for i,(st,en) in enumerate(segments):\n",
        "            seg = y_proc[..., st:en]\n",
        "            if params['normalize']!=\"No Normalization\":\n",
        "                log(f\"Normalizing {'segment '+str(i+1) if cfg.segmentation else 'file'}\")\n",
        "                seg, _ = normalize_loudness(seg, cfg.sample_rate, log, cfg.target_lufs, cfg.target_peak)\n",
        "            out_path = export_audio(seg, cfg.sample_rate, fp, params['out_fmt'], cfg, OUTPUT_DIR, idx=(i if cfg.segmentation else None))\n",
        "            res['export_paths'].append(out_path)\n",
        "            log(f\"‚úÖ Exported {os.path.basename(out_path)}\")\n",
        "        return res\n",
        "\n",
        "    except Exception:\n",
        "        tb = traceback.format_exc()\n",
        "        res['messages'].append(f\"‚ùå ERROR:\\n{tb}\")\n",
        "        return res\n",
        "\n",
        "def gradio_process(input_mode, uploads, path_in, gdrive_url,\n",
        "                  out_fmt, sr, bd, ch, mp3_br,\n",
        "                  norm, pan, seg, dur, tu, viz,\n",
        "                  zip_enable, custom_zip_name, silence_trimming):  # Added silence_trimming parameter\n",
        "    logs = [\"=== Input Method ===\"]\n",
        "    raw_inputs = []\n",
        "    base_name_for_zip = None\n",
        "\n",
        "    if input_mode==\"Path\" and path_in.strip():\n",
        "        logs.append(f\"Mode: Path ‚ûû {path_in}\")\n",
        "        raw_inputs = get_all_audio_files(path_in.strip())\n",
        "        base_name_for_zip = os.path.basename(path_in.rstrip(\"/\"))\n",
        "    elif input_mode==\"Google Drive URL\" and gdrive_url.strip():\n",
        "        downloaded, detected = download_from_gdrive_folder(gdrive_url.strip(), logs.append)\n",
        "        if not downloaded:\n",
        "            return \"\\n\".join(logs+[\"‚ùå Aborting: invalid Google Drive URL.\"]), [], [], gr.update(choices=[], value=None), None\n",
        "        logs.append(f\"Mode: Google Drive URL ‚ûû {gdrive_url}\")\n",
        "        base_name_for_zip = detected\n",
        "        for root,_,_ in os.walk(downloaded):\n",
        "            raw_inputs.extend(get_all_audio_files(root))\n",
        "    else:\n",
        "        logs.append(f\"Mode: Upload ‚ûû {len(uploads) if uploads else 0} file(s)\")\n",
        "        raw_inputs = [f.name for f in uploads] if uploads else []\n",
        "        base_name_for_zip = None\n",
        "\n",
        "    logs.append(\"=== Original File Details ===\")\n",
        "    for fp in raw_inputs:\n",
        "        try:\n",
        "            info = sf.info(fp)\n",
        "            logs.append(f\"{os.path.basename(fp)}: {info.samplerate}Hz, {info.channels}ch, {format_duration(info.duration)}\")\n",
        "        except:\n",
        "            logs.append(f\"{os.path.basename(fp)}: <could not read metadata>\")\n",
        "    logs.append(\"\")\n",
        "\n",
        "    logs.append(\"=== Output Settings ===\")\n",
        "    logs.append(f\"Format: {out_fmt}\")\n",
        "    logs.append(f\"Sample Rate: {sr}\")\n",
        "    if out_fmt.lower()==\"mp3\":\n",
        "        logs.append(f\"MP3 Bitrate: {mp3_br}\")\n",
        "    else:\n",
        "        logs.append(f\"Bit Depth: {bd}\")\n",
        "    logs.append(f\"Channels: {ch}\")\n",
        "    logs.append(\"\")\n",
        "\n",
        "    export_bd = bd\n",
        "    if out_fmt==\"flac\" and bd==\"32\":\n",
        "        logs.append(\"‚ö† FLAC does not support 32-bit; falling back to 24-bit\")\n",
        "        export_bd = \"24\"\n",
        "\n",
        "    logs.append(\"=== Processing Options ===\")\n",
        "    logs.append(f\"Normalization Profile: {norm}\")\n",
        "    logs.append(f\"Panning Correction: {pan}\")\n",
        "    logs.append(f\"Silence Trimming: {silence_trimming}\")  # New log entry\n",
        "    logs.append(f\"Segmentation: {'Yes' if seg else 'No'}\" + (f\", Duration {dur}{tu}\" if seg else \"\"))\n",
        "    logs.append(f\"Show Visualizations: {'Yes' if viz else 'No'}\")\n",
        "    logs.append(f\"Custom ZIP Name: {custom_zip_name or '(none)'}\")\n",
        "    logs.append(\"\")\n",
        "\n",
        "    if not raw_inputs:\n",
        "        return \"\\n\".join(logs+[\"‚ùå No audio files provided.\"]), [], [], gr.update(choices=[], value=None), None\n",
        "\n",
        "    params = {'out_fmt': out_fmt, 'normalize': norm}\n",
        "    tgt_lufs, tgt_peak = {\"Spotify\":(-14.0,-1.0), \"Apple Music\":(-16.0,-1.0)}.get(norm, (None, None))\n",
        "    raw = float(dur)\n",
        "    if tu==\"Milliseconds\": dsec = raw/1000\n",
        "    elif tu==\"Minutes\":\n",
        "        dsec = raw*60\n",
        "    elif tu==\"Hours\":\n",
        "        dsec = raw*3600\n",
        "    else:\n",
        "        dsec = raw\n",
        "\n",
        "    cfg = Config(\n",
        "        sample_rate=int(sr.replace(\"Hz\",\"\")),\n",
        "        bit_depth=export_bd, channels=ch,\n",
        "        target_lufs=tgt_lufs, target_peak=tgt_peak,\n",
        "        use_cuda=use_cuda, device=device,\n",
        "        visualize=viz, segmentation=seg,\n",
        "        duration=dsec, panning=(pan==\"Yes\"),\n",
        "        mp3_bitrate=mp3_br,\n",
        "        silence_trimming=(silence_trimming==\"Yes\")  # New config parameter\n",
        "    )\n",
        "\n",
        "    gallery_images = []\n",
        "    export_paths = []\n",
        "\n",
        "    workers = os.cpu_count() or 1\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "        futures = {executor.submit(process_file, fp, cfg, params): fp for fp in raw_inputs}\n",
        "        for fut in concurrent.futures.as_completed(futures):\n",
        "            r = fut.result()\n",
        "            logs.extend(r['messages'])\n",
        "            logs.append(\"\")\n",
        "            export_paths.extend(r['export_paths'])\n",
        "            if viz and r.get('plot_data'):\n",
        "                pd = r['plot_data']\n",
        "                figs = [\n",
        "                    (plot_waveform(pd['y_pre'], pd['sr'], \"Original w/ Trim\", tu[0], pd['s0'], pd['e0'], pd['segments']), 'pre'),\n",
        "                    (plot_waveform(pd['y_proc'], pd['sr'], \"Processed Output\", tu[0]), 'post'),\n",
        "                    (plot_zoomed_silence(pd['y_pre'], pd['sr'], pd['s0'], pd['e0']), 'zoom')\n",
        "                ]\n",
        "                for fig, tag in figs:\n",
        "                    fn = os.path.join(OUTPUT_DIR, f\"{uuid4().hex}_{tag}.png\")\n",
        "                    fig.savefig(fn)\n",
        "                    gallery_images.append(fn)\n",
        "\n",
        "    play_paths = []\n",
        "    for p in export_paths:\n",
        "        if p.lower().endswith('.flac'):\n",
        "            wav_play = p[:-5] + '_playback.wav'\n",
        "            y, sr_load = sf.read(p)\n",
        "            sf.write(wav_play, y, sr_load)\n",
        "            play_paths.append(wav_play)\n",
        "        else:\n",
        "            play_paths.append(p)\n",
        "\n",
        "    if zip_enable and export_paths:\n",
        "        if custom_zip_name.strip():\n",
        "            zip_base = custom_zip_name.strip()\n",
        "        elif base_name_for_zip:\n",
        "            zip_base = base_name_for_zip\n",
        "        else:\n",
        "            zip_base = os.path.splitext(os.path.basename(raw_inputs[0]))[0]\n",
        "        zip_filename = f\"{zip_base}.zip\"\n",
        "        zip_path = os.path.join(OUTPUT_DIR, zip_filename)\n",
        "        with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "            for ex in export_paths:\n",
        "                arc = os.path.relpath(ex, OUTPUT_DIR)\n",
        "                zf.write(ex, arcname=arc)\n",
        "        download_paths = [zip_path]\n",
        "    else:\n",
        "        download_paths = export_paths\n",
        "\n",
        "    logs.append(\"=== Exported Files ===\")\n",
        "    logs.append(f\"Count: {len(download_paths)}\")\n",
        "    logs.append(\"--- Used Settings ---\")\n",
        "    logs.append(f\"Format: {out_fmt}\")\n",
        "    logs.append(f\"Sample Rate: {sr}\")\n",
        "    if out_fmt.lower()==\"mp3\":\n",
        "        logs.append(f\"MP3 Bitrate: {mp3_br}\")\n",
        "    else:\n",
        "        logs.append(f\"Bit Depth: {export_bd}\")\n",
        "    logs.append(f\"Channels: {ch}\")\n",
        "    logs.append(f\"Normalization Profile: {norm}\")\n",
        "    logs.append(f\"Panning Correction: {pan}\")\n",
        "    logs.append(f\"Silence Trimming: {silence_trimming}\")  # New log entry\n",
        "    logs.append(f\"Segmentation: {'Yes' if seg else 'No'}\" + (f\", Duration {dur}{tu}\" if seg else \"\"))\n",
        "    logs.append(f\"Visualizations: {'Yes' if viz else 'No'}\")\n",
        "    logs.append(\"\")\n",
        "    for fn in download_paths:\n",
        "        logs.append(os.path.basename(fn))\n",
        "\n",
        "    default_play = play_paths[0] if play_paths else None\n",
        "    dropdown_update = gr.update(choices=play_paths, value=default_play)\n",
        "\n",
        "    return (\n",
        "        \"\\n\".join(logs),\n",
        "        gallery_images,\n",
        "        download_paths,\n",
        "        dropdown_update,\n",
        "        default_play\n",
        "    )\n",
        "\n",
        "# === Gradio UI ===\n",
        "with gr.Blocks(title=\"Audio Preprocessor GUI\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Audio Preprocessor\")\n",
        "    gr.Markdown(f\"Outputs saved in `{OUTPUT_DIR}`\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            input_mode     = gr.Radio([\"Upload\",\"Path\",\"Google Drive URL\"], value=\"Upload\", label=\"Input Method\")\n",
        "            file_uploader  = gr.File(file_count=\"multiple\", file_types=[\"audio\"], label=\"Upload Audio Files\")\n",
        "            path_text      = gr.Textbox(placeholder=\"/path/to/dir\", label=\"Or Enter Path\", visible=False)\n",
        "            gdrive_text    = gr.Textbox(placeholder=\"URL Link\", label=\"Or Enter Google Drive FOLDER URL\", visible=False)\n",
        "            input_mode.change(\n",
        "                lambda m: (\n",
        "                    gr.update(visible=(m==\"Upload\")),\n",
        "                    gr.update(visible=(m==\"Path\")),\n",
        "                    gr.update(visible=(m==\"Google Drive URL\"))\n",
        "                ),\n",
        "                inputs=[input_mode],\n",
        "                outputs=[file_uploader, path_text, gdrive_text]\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### Output Settings\")\n",
        "            with gr.Row():\n",
        "                out_fmt     = gr.Dropdown([\"wav\",\"mp3\",\"flac\",\"aiff\"],    value=\"wav\",      label=\"Format\")\n",
        "                sample_rate = gr.Dropdown([\"16000Hz\",\"44100Hz\",\"48000Hz\"], value=\"48000Hz\", label=\"Sample Rate\")\n",
        "                bit_depth   = gr.Dropdown([\"16\",\"24\",\"32\"],               value=\"24\",       label=\"Bit Depth\")\n",
        "            with gr.Row():\n",
        "                channels    = gr.Radio([\"mono\",\"stereo\"],                value=\"mono\",     label=\"Channels\")\n",
        "                mp3_bitrate = gr.Dropdown([\"128k\",\"192k\",\"256k\",\"320k\"],  value=\"192k\",     label=\"MP3 Bitrate\")\n",
        "\n",
        "    with gr.Accordion(\"Processing Options\", open=True):\n",
        "        with gr.Row():\n",
        "            norm_profile   = gr.Dropdown([\"No Normalization\",\"Spotify\",\"Apple Music\"], value=\"Spotify\", label=\"Normalization Profile\")\n",
        "            panning_option = gr.Dropdown([\"Yes\",\"No\"],                             value=\"Yes\",     label=\"Enable Panning Correction\")\n",
        "            silence_trimming = gr.Dropdown([\"Yes\",\"No\"],                           value=\"Yes\",     label=\"Enable Silence Trimming\")  # New dropdown\n",
        "            visualize      = gr.Checkbox(value=True, label=\"Show Visualizations\")\n",
        "        with gr.Row():\n",
        "            segmentation = gr.Checkbox(value=False, label=\"Enable Segmentation\")\n",
        "            duration     = gr.Slider(minimum=1, maximum=60, step=1, value=30, label=\"Segment Duration\")\n",
        "            time_unit    = gr.Dropdown([\"Milliseconds\",\"Seconds\",\"Minutes\",\"Hours\"], value=\"Seconds\", label=\"Time Unit\")\n",
        "        with gr.Row():\n",
        "            zip_enable      = gr.Checkbox(value=True, label=\"Save outputs as ZIP\")\n",
        "            custom_zip_name = gr.Textbox(placeholder=\"Enter ZIP name (without .zip)\", label=\"Custom ZIP Name (optional)\")\n",
        "\n",
        "    process_btn = gr.Button(\"Process Audio\", variant=\"primary\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Logs\"):\n",
        "            logs_out = gr.Textbox(lines=15, label=\"Processing Logs\", interactive=False)\n",
        "        with gr.TabItem(\"Visualizations\"):\n",
        "            gr.Markdown(\"All waveform plots (3 per file)\")\n",
        "            gallery  = gr.Gallery(label=\"Plots\", columns=3, height=\"auto\")\n",
        "        with gr.TabItem(\"Output Files\"):\n",
        "            audio_out = gr.File(file_count=\"multiple\", label=\"Processed Audio Files\", interactive=False)\n",
        "        with gr.TabItem(\"Audio Player\"):\n",
        "            file_selector = gr.Dropdown(choices=[], label=\"Select File to Play\")\n",
        "            audio_player  = gr.Audio(label=\"Play Processed Audio\", interactive=True)\n",
        "\n",
        "    process_btn.click(\n",
        "        fn=gradio_process,\n",
        "        inputs=[\n",
        "            input_mode, file_uploader, path_text, gdrive_text,\n",
        "            out_fmt, sample_rate, bit_depth, channels, mp3_bitrate,\n",
        "            norm_profile, panning_option, segmentation, duration,\n",
        "            time_unit, visualize, zip_enable, custom_zip_name,\n",
        "            silence_trimming  # Added new input\n",
        "        ],\n",
        "        outputs=[logs_out, gallery, audio_out, file_selector, audio_player]\n",
        "    )\n",
        "\n",
        "    file_selector.change(fn=lambda f: f, inputs=file_selector, outputs=audio_player)\n",
        "\n",
        "    # Enable Gradio queueing to avoid HTTP timeouts\n",
        "    demo.queue()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "i0h71GKL1yri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_-xs_6srjuj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Beat Identifier UI\n",
        "\n",
        "# === Auto-install Required Packages ===\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install(pkg):\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "required_packages = {\n",
        "    \"gradio\": \"gradio\",\n",
        "    \"librosa\": \"librosa\",\n",
        "    \"numpy\": \"numpy\",\n",
        "    \"torch\": \"torch\",\n",
        "    \"scipy\": \"scipy\"\n",
        "}\n",
        "\n",
        "for pkg_name, pip_name in required_packages.items():\n",
        "    install(pkg_name)\n",
        "\n",
        "# === Main Script ===\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "import socket\n",
        "import gradio as gr\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import scipy.signal\n",
        "from scipy.signal.windows import hann as _hann\n",
        "\n",
        "# Patch SciPy so librosa.beat_track can find hann()\n",
        "scipy.signal.hann = _hann\n",
        "\n",
        "# Detect device\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "notes = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n",
        "\n",
        "def analyze_single(path):\n",
        "    fn = os.path.basename(path)\n",
        "    try:\n",
        "        y, sr = librosa.load(path, sr=None)\n",
        "        if y.size == 0:\n",
        "            return f\"Error: {fn} is empty.\", None\n",
        "        if use_cuda:\n",
        "            import torch as _t\n",
        "            y = _t.from_numpy(y).to(device).cpu().numpy()\n",
        "\n",
        "        dur = librosa.get_duration(y=y, sr=sr)\n",
        "        if dur == 0:\n",
        "            return f\"Error: {fn} duration is zero.\", None\n",
        "        m, s = divmod(int(dur), 60)\n",
        "\n",
        "        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "        bpm = 0 if onset_env.sum() == 0 else int(round(librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)[0]))\n",
        "\n",
        "        chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
        "        key = notes[int(np.mean(chroma, axis=1).argmax())]\n",
        "\n",
        "        tuning = librosa.estimate_tuning(y=y, sr=sr)\n",
        "\n",
        "        summary = (\n",
        "            f\"**{fn}**\\n\\n\"\n",
        "            f\"- Duration: {m}m {s}s ({dur:.2f}s)\\n\"\n",
        "            f\"- BPM: {bpm if bpm > 0 else '‚Äî'}\\n\"\n",
        "            f\"- Key: {key}\\n\"\n",
        "            f\"- Tuning offset: {tuning:.3f} semitones\"\n",
        "        )\n",
        "        return summary, path\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error analyzing {fn}: {e}\", None\n",
        "\n",
        "def analyze_batch(paths):\n",
        "    with ThreadPoolExecutor(max_workers=min(4, os.cpu_count())) as ex:\n",
        "        results = list(ex.map(analyze_single, paths))\n",
        "    summaries, out_paths, opts = [], [], []\n",
        "    for summary, p in results:\n",
        "        summaries.append(summary)\n",
        "        if p:\n",
        "            out_paths.append(p)\n",
        "            opts.append((os.path.basename(p), p))\n",
        "    return \"\\n\\n---\\n\\n\".join(summaries), out_paths, opts\n",
        "\n",
        "def find_available_port(start=7860, end=7900):\n",
        "    for port in range(start, end + 1):\n",
        "        with socket.socket() as s:\n",
        "            try:\n",
        "                s.bind((\"\", port))\n",
        "                return port\n",
        "            except OSError:\n",
        "                continue\n",
        "    return start\n",
        "\n",
        "def process_upload(files):\n",
        "    if not files:\n",
        "        return \"‚ö† Please upload one or more MP3/WAV files.\", [], []\n",
        "    paths = [f.name for f in files]\n",
        "    summary, out_paths, opts = analyze_batch(paths)\n",
        "    default = out_paths[0] if out_paths else None\n",
        "    return summary, out_paths, gr.update(choices=opts, value=default)\n",
        "\n",
        "# === Build GUI ===\n",
        "with gr.Blocks(title=\"Beat Identifier\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Beat Identifier\")\n",
        "    gr.Markdown(\"Drag & drop your MP3/WAV files below\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            upload = gr.File(\n",
        "                label=\"Upload Audio Files\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".mp3\", \".wav\"],\n",
        "                type=\"file\"\n",
        "            )\n",
        "            btn = gr.Button(\"Analyze Audio\", variant=\"primary\")\n",
        "        with gr.Column(scale=3):\n",
        "            gr.Markdown(\"### Results\")\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Summary\"):\n",
        "            out_md = gr.Markdown(\"\", label=\"Analysis Summary\")\n",
        "        with gr.TabItem(\"Files\"):\n",
        "            out_files = gr.File(file_count=\"multiple\", label=\"Download Files\")\n",
        "        with gr.TabItem(\"Playback\"):\n",
        "            selector = gr.Dropdown(choices=[], label=\"Select File to Play\")\n",
        "            audio_player = gr.Audio(label=\"Preview\", interactive=True)\n",
        "\n",
        "    selector.change(lambda p: p or None, selector, audio_player)\n",
        "    btn.click(process_upload, inputs=[upload], outputs=[out_md, out_files, selector])\n",
        "\n",
        "    demo.queue()  # server-side queue to avoid mobile disconnects\n",
        "\n",
        "# === Launch ===\n",
        "if __name__ == \"__main__\":\n",
        "    port = find_available_port()\n",
        "    in_colab = False\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        in_colab = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    demo.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=port,\n",
        "        share=in_colab,   # must share=True in Colab when using queue()\n",
        "        debug=in_colab    # show errors inline in Colab\n",
        "    )"
      ]
    }
  ]
}